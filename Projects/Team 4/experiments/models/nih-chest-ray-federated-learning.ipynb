{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import torch\n","import numpy as np\n","import pandas as pd\n","import torchvision.models as models\n","import torchvision.transforms as trns\n","from torchvision.datasets import ImageFolder\n","from torchvision.transforms import ToTensor\n","from torch.utils.data import random_split, DataLoader, Dataset\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = [8, 10]"]},{"cell_type":"markdown","metadata":{},"source":["## Map images to their location"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["file_map = dict()\n","for i in range(12):\n","    path = '../datasets/nih-chest-xray/input/data/images_' + ('00' if i < 9 else '0') + str(i + 1) + '/images/'\n","    for f in os.listdir(path):\n","        if os.path.isfile(os.path.join(path, f)) and f[-4:] == '.png':\n","            file_map[f] = os.path.join(path, f)"]},{"cell_type":"markdown","metadata":{},"source":["## Load dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df = pd.read_csv('../datasets/nih-chest-xray/input/data/Data_Entry_2017.csv')\n","df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def showImage(pil):\n","    plt.imshow(np.array(pil) / 255)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def loadImage(path):\n","    return Image.open(path).convert('RGB')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["showImage(loadImage(file_map[list(file_map.keys())[100]]))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["classes = [\n","    'Atelectasis', \n","    'Consolidation', \n","    'Infiltration', \n","    'Pneumothorax', \n","    'Edema', \n","    'Emphysema', \n","    'Fibrosis', \n","    'Effusion', \n","    'Pneumonia', \n","    'Pleural_thickening', \n","    'Cardiomegaly', \n","    'Nodule', \n","    'Mass', \n","    'Hernia', \n","    'No Finding'\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class ImageDataset(Dataset):\n","    def __init__(self, data, transforms):\n","        self.image_paths = [file_map[f] for f in data[0]]\n","        self.labels = data[1]\n","        self.transforms = transforms\n","        \n","    def __len__(self):\n","        return len(data[0])\n","    \n","    def __getitem__(self, idx):\n","        image = self.transforms(loadImage(self.image_paths[idx]))\n","        target = torch.tensor([int(cls in self.labels[idx]) for cls in classes], dtype=torch.float32)\n","        return (image, target)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data = (df.iloc[:5000, 0], [df.iloc[i, 1].split('|') for i in range(5000)])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dataset = ImageDataset(data, trns.Compose([\n","    trns.Resize((240, 240)),\n","    trns.ToTensor(), \n","    trns.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225],inplace=True)\n","]))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(dataset[1][1])\n","plt.imshow(dataset[1][0].permute((1, 2, 0)))"]},{"cell_type":"markdown","metadata":{},"source":["## Define training and validation sets"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataset, validation_dataset = random_split(dataset, [int(len(dataset) * 0.85), \n","            len(dataset) - int(len(dataset) * 0.85)])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataset_size = len(train_dataset)\n","validation_dataset_size = len(validation_dataset)\n","\n","train_dataset_size, validation_dataset_size"]},{"cell_type":"markdown","metadata":{},"source":["## Define notebook constants"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["num_classes = 15\n","input_shape = (3, 240, 240)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["num_clients = 3\n","rounds = 10\n","batch_size = 64\n","epochs_per_client = 6\n","learning_rate = 1e-1"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["resnet34 = models.resnet34(pretrained=True)\n","resnet34"]},{"cell_type":"markdown","metadata":{},"source":["## Define GPU utilities"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_device():\n","    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","def to_device(data, device):\n","    if isinstance(data, (list, tuple)):\n","        return [to_device(x, device) for x in data]\n","    return data.to(device, non_blocking=True)\n","\n","class DeviceDataLoader(DataLoader):\n","        def __init__(self, dl, device):\n","            self.dl = dl\n","            self.device = device\n","\n","        def __iter__(self):\n","            for batch in self.dl:\n","                yield to_device(batch, self.device)\n","\n","        def __len__(self):\n","            return len(self.dl)\n","\n","device = get_device()"]},{"cell_type":"markdown","metadata":{},"source":["## Define FederatedNet class"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class FederatedNet(torch.nn.Module):    \n","    def __init__(self):\n","        super().__init__()\n","        self.network = models.resnet34(pretrained=True)\n","        self.network.fc = torch.nn.Linear(self.network.fc.in_features, num_classes)\n","        self.track_layers = {\n","            'layer4':  self.network.layer4,\n","            'linear': self.network.fc\n","        }\n","        self.freeze()\n","\n","    def freeze(self):\n","        for param in self.network.parameters():\n","            param.requires_grad = False\n","        for layer_name in self.track_layers:\n","            for param in self.track_layers[layer_name].parameters():\n","                param.requires_grad = True\n","    \n","    def forward(self, x_batch):\n","        out = torch.sigmoid(self.network(x_batch))\n","        return out\n","    \n","    def get_track_layers(self):\n","        return self.track_layers\n","    \n","    def apply_parameters(self, parameters_dict):\n","        with torch.no_grad():\n","            for layer_name in parameters_dict:\n","                layer_params = list(self.track_layers[layer_name].parameters())\n","                for i in range(len(layer_params)):\n","                    layer_params[i].data = (layer_params[i].data + (parameters_dict[layer_name][i] - \n","                                                layer_params[i].data))\n","    \n","    def get_parameters(self):\n","        parameters_dict = dict()\n","        for layer_name in self.track_layers:\n","            parameters_dict[layer_name] = [param.data.clone().detach() for param in self.track_layers\n","                                                [layer_name].parameters()]\n","        return parameters_dict\n","    \n","    def batch_accuracy(self, outputs, labels):\n","        with torch.no_grad():\n","            return torch.tensor(torch.sum((outputs > 0.5) == labels).item() / len(outputs))\n","    \n","    def _process_batch(self, batch):\n","        images, labels = batch\n","        outputs = self(images)\n","        loss = torch.nn.functional.binary_cross_entropy(outputs, labels)\n","        accuracy = self.batch_accuracy(outputs, labels)\n","        return (loss, accuracy)\n","    \n","    def fit(self, dataset, epochs, lr, batch_size=128, opt=torch.optim.SGD):\n","        self.train()\n","        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size, shuffle=True), device)\n","        optimizer = opt(self.parameters(), lr)\n","        history = []\n","        for epoch in range(epochs):\n","            losses = []\n","            accs = []\n","            for batch in dataloader:\n","                loss, acc = self._process_batch(batch)\n","                loss.backward()\n","                optimizer.step()\n","                optimizer.zero_grad()\n","                loss.detach()\n","                losses.append(loss)\n","                accs.append(acc)\n","            avg_loss = torch.stack(losses).mean().item()\n","            avg_acc = torch.stack(accs).mean().item()\n","            history.append((avg_loss, avg_acc))\n","        return history\n","    \n","    def evaluate(self, dataset, batch_size=64):\n","        self.eval()\n","        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size), device)\n","        losses = []\n","        accs = []\n","        with torch.no_grad():\n","            for batch in dataloader:\n","                loss, acc = self._process_batch(batch)\n","                losses.append(loss)\n","                accs.append(acc)\n","        avg_loss = torch.stack(losses).mean().item()\n","        avg_acc = torch.stack(accs).mean().item()\n","        return (avg_loss, avg_acc)"]},{"cell_type":"markdown","metadata":{},"source":["## Define Client class"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class Client:\n","    def __init__(self, client_id, dataset):\n","        self.client_id = client_id\n","        self.dataset = dataset\n","    \n","    def get_dataset_size(self):\n","        return len(self.dataset)\n","    \n","    def get_client_id(self):\n","        return self.client_id\n","    \n","    def train(self, parameters_dict, return_model_dict=False):\n","        net = to_device(FederatedNet(), device)\n","        net.apply_parameters(parameters_dict)\n","        train_history = net.fit(self.dataset, epochs_per_client, learning_rate, batch_size)\n","        print(self.client_id + ':')\n","        for i, res in enumerate(train_history):\n","            print('Epoch [{}]: Loss = {}, Accuracy = {}'.format(i + 1, round(res[0], 4), round(res[1], 4)))\n","        return net.get_parameters(), net.state_dict() if return_model_dict else None"]},{"cell_type":"markdown","metadata":{},"source":["## Setup clients"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["examples_per_client = train_dataset_size // num_clients\n","client_datasets = random_split(train_dataset, [min(i + examples_per_client, \n","           train_dataset_size) - i for i in range(0, train_dataset_size, examples_per_client)])\n","clients = [Client('client_' + str(i), client_datasets[i]) for i in range(num_clients)]"]},{"cell_type":"markdown","metadata":{},"source":["## Start server"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["global_net = to_device(FederatedNet(), device)\n","history = []\n","\n","for i in range(rounds):\n","    print('Start Round {} ...'.format(i + 1))\n","    curr_parameters = global_net.get_parameters()\n","    new_parameters = dict([(layer_name, [0 for param in global_net.track_layers[layer_name].parameters()])\n","                            for layer_name in curr_parameters])\n","    for j, client in enumerate(clients):\n","        client_parameters, state_dict = client.train(curr_parameters, (j == len(clients) - 1))\n","        if j == len(clients) - 1:\n","            global_net.load_state_dict(state_dict)\n","            global_net.apply_parameters(client_parameters)\n","            train_loss, train_acc = global_net.evaluate(train_dataset)\n","            val_loss, val_acc = global_net.evaluate(validation_dataset)\n","            print('After round {}, train_loss = {}, val_loss = {}, val_acc = {}\\n'.format(i + 1, round(train_loss, 4), \n","                    round(val_loss, 4), round(val_acc, 4)))\n","            history.append((train_loss, val_loss))\n","\n","        fraction = client.get_dataset_size() / train_dataset_size\n","        for layer_name in client_parameters:\n","            for j in range(len(client_parameters[layer_name])):\n","                new_parameters[layer_name][j] += fraction * client_parameters[layer_name][j]\n","\n","    global_net.apply_parameters(new_parameters)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.plot([i + 1 for i in range(len(history))], [history[i][0] for i in range(len(history))], color='r', label='train loss')\n","plt.plot([i + 1 for i in range(len(history))], [history[i][1] for i in range(len(history))], color='b', label='val loss')\n","plt.title('Training history')\n","plt.legend()\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}
