{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from ref: https://github.com/pytorch/examples/blob/main/mnist/main.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from model import Net\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA A100-SXM4-80GB'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval=100, dry_run=False):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if dry_run:\n",
    "                break\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "# use_mps = torch.backends.mps.is_available()\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "test_batch_size=1000\n",
    "train_kwargs = {'batch_size': batch_size}\n",
    "test_kwargs = {'batch_size': test_batch_size}\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                   'pin_memory': True,\n",
    "                   'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr=1e-2\n",
    "epochs=20\n",
    "gamma=0.7\n",
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,\n",
    "                   transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.323007\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.583624\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.992508\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.819872\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.714190\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.523111\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.406433\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.666980\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.371039\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.493222\n",
      "\n",
      "Test set: Average loss: 0.2943, Accuracy: 9157/10000 (92%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.510855\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.640901\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.263283\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.379489\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.284875\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.348771\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.356998\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.325492\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.137533\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.339891\n",
      "\n",
      "Test set: Average loss: 0.2302, Accuracy: 9342/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.366555\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.318184\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.427572\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.270446\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.204995\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.247255\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.232944\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.174689\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.363594\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.331155\n",
      "\n",
      "Test set: Average loss: 0.2052, Accuracy: 9410/10000 (94%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.363403\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.345614\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.312848\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.307685\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.214207\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.254916\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.293233\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.376887\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.340018\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.255609\n",
      "\n",
      "Test set: Average loss: 0.1918, Accuracy: 9448/10000 (94%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.286247\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.267557\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.352303\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.302103\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.244281\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.153205\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.256762\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.279974\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.263400\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.144384\n",
      "\n",
      "Test set: Average loss: 0.1831, Accuracy: 9465/10000 (95%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.179257\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.244045\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.207106\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.197009\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.391155\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.218197\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.404099\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.144373\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.384381\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.186842\n",
      "\n",
      "Test set: Average loss: 0.1768, Accuracy: 9484/10000 (95%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.432607\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.388291\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.258007\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.338227\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.447245\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.244096\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.415495\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.290644\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.289095\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.181185\n",
      "\n",
      "Test set: Average loss: 0.1731, Accuracy: 9497/10000 (95%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.250525\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.247766\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.266971\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.294639\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.232312\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.160666\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.323720\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.149059\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.126316\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.384234\n",
      "\n",
      "Test set: Average loss: 0.1705, Accuracy: 9501/10000 (95%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.308468\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.274819\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.126883\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.211562\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.358075\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.219877\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.335950\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.249869\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.100471\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.381307\n",
      "\n",
      "Test set: Average loss: 0.1686, Accuracy: 9504/10000 (95%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.182099\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.312455\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.277861\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.303662\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.162408\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.292368\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.313147\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.338436\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.319554\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.189224\n",
      "\n",
      "Test set: Average loss: 0.1676, Accuracy: 9507/10000 (95%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.172900\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.354422\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.306002\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.283657\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.185410\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.420091\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.285502\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.337027\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.311818\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.227548\n",
      "\n",
      "Test set: Average loss: 0.1669, Accuracy: 9514/10000 (95%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.193336\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.182128\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.312830\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.133633\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.286732\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.132718\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.160056\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.243057\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.264579\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.118275\n",
      "\n",
      "Test set: Average loss: 0.1659, Accuracy: 9510/10000 (95%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.343569\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.144602\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.152610\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.192969\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.283388\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.235542\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.137297\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.288944\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.241894\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.213960\n",
      "\n",
      "Test set: Average loss: 0.1655, Accuracy: 9511/10000 (95%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.093331\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.273631\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.311406\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.316938\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.265632\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.207988\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.216656\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.180920\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.406507\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.172251\n",
      "\n",
      "Test set: Average loss: 0.1654, Accuracy: 9511/10000 (95%)\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.322080\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 0.203159\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.358729\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 0.359686\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.309639\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.429456\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.316466\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 0.133801\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.200143\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 0.247719\n",
      "\n",
      "Test set: Average loss: 0.1652, Accuracy: 9514/10000 (95%)\n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.183797\n",
      "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 0.127084\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 0.252422\n",
      "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 0.293287\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.324763\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 0.225200\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.252707\n",
      "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 0.245087\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.286225\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 0.302595\n",
      "\n",
      "Test set: Average loss: 0.1650, Accuracy: 9513/10000 (95%)\n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.239235\n",
      "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 0.244240\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 0.212579\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 0.230522\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.141884\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 0.449479\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.450662\n",
      "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 0.321226\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.217815\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 0.278206\n",
      "\n",
      "Test set: Average loss: 0.1650, Accuracy: 9513/10000 (95%)\n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.300657\n",
      "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 0.442859\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 0.256259\n",
      "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 0.201901\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.210006\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 0.378131\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 0.305467\n",
      "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 0.419910\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.283330\n",
      "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 0.403881\n",
      "\n",
      "Test set: Average loss: 0.1649, Accuracy: 9516/10000 (95%)\n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.125246\n",
      "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 0.262562\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 0.282515\n",
      "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 0.315599\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.277063\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 0.265848\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 0.302174\n",
      "Train Epoch: 19 [44800/60000 (75%)]\tLoss: 0.156839\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.208407\n",
      "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 0.331068\n",
      "\n",
      "Test set: Average loss: 0.1649, Accuracy: 9516/10000 (95%)\n",
      "\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.379676\n",
      "Train Epoch: 20 [6400/60000 (11%)]\tLoss: 0.074603\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 0.306680\n",
      "Train Epoch: 20 [19200/60000 (32%)]\tLoss: 0.207799\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.162728\n",
      "Train Epoch: 20 [32000/60000 (53%)]\tLoss: 0.191980\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 0.213317\n",
      "Train Epoch: 20 [44800/60000 (75%)]\tLoss: 0.172609\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.226478\n",
      "Train Epoch: 20 [57600/60000 (96%)]\tLoss: 0.182293\n",
      "\n",
      "Test set: Average loss: 0.1649, Accuracy: 9516/10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './vanilla_pytorch_mnist_'+f'{torch.cuda.get_device_name(0)}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=torch.load('./vanilla_pytorch_mnist_'+f'{torch.cuda.get_device_name(0)}.pth')\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_elapsed(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        t0 = time.perf_counter()\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        time_elapsed = time.perf_counter() - t0\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    print('Time Elapsed:{}'.format(time_elapsed))\n",
    "    return time_elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1649, Accuracy: 9516/10000 (95%)\n",
      "\n",
      "Time Elapsed:1.1684837199991307\n",
      "\n",
      "Test set: Average loss: 0.1649, Accuracy: 9516/10000 (95%)\n",
      "\n",
      "Time Elapsed:1.1432125230003294\n",
      "\n",
      "Test set: Average loss: 0.1649, Accuracy: 9516/10000 (95%)\n",
      "\n",
      "Time Elapsed:1.1396725149988924\n",
      "\n",
      "Test set: Average loss: 0.1649, Accuracy: 9516/10000 (95%)\n",
      "\n",
      "Time Elapsed:1.1414269739998417\n",
      "\n",
      "Test set: Average loss: 0.1649, Accuracy: 9516/10000 (95%)\n",
      "\n",
      "Time Elapsed:1.1450750989988592\n",
      "\n",
      "Test set: Average loss: 0.1649, Accuracy: 9516/10000 (95%)\n",
      "\n",
      "Time Elapsed:1.1489928430000873\n",
      "\n",
      "Test set: Average loss: 0.1649, Accuracy: 9516/10000 (95%)\n",
      "\n",
      "Time Elapsed:1.1478785049985163\n",
      "\n",
      "Test set: Average loss: 0.1649, Accuracy: 9516/10000 (95%)\n",
      "\n",
      "Time Elapsed:1.1400177989999065\n",
      "\n",
      "Test set: Average loss: 0.1649, Accuracy: 9516/10000 (95%)\n",
      "\n",
      "Time Elapsed:1.1422827429996687\n",
      "\n",
      "Test set: Average loss: 0.1649, Accuracy: 9516/10000 (95%)\n",
      "\n",
      "Time Elapsed:1.1445842850007466\n"
     ]
    }
   ],
   "source": [
    "nruns=10\n",
    "time_dict={key:0 for key in range(nruns)}\n",
    "for i in range(nruns):\n",
    "    time_dict[i]=get_time_elapsed(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.1684837199991307,\n",
       " 1: 1.1432125230003294,\n",
       " 2: 1.1396725149988924,\n",
       " 3: 1.1414269739998417,\n",
       " 4: 1.1450750989988592,\n",
       " 5: 1.1489928430000873,\n",
       " 6: 1.1478785049985163,\n",
       " 7: 1.1400177989999065,\n",
       " 8: 1.1422827429996687,\n",
       " 9: 1.1445842850007466}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>time_elapsed</th>\n",
       "      <th>device</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.168484</td>\n",
       "      <td>NVIDIA A100-SXM4-80GB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.143213</td>\n",
       "      <td>NVIDIA A100-SXM4-80GB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.139673</td>\n",
       "      <td>NVIDIA A100-SXM4-80GB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.141427</td>\n",
       "      <td>NVIDIA A100-SXM4-80GB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.145075</td>\n",
       "      <td>NVIDIA A100-SXM4-80GB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1.148993</td>\n",
       "      <td>NVIDIA A100-SXM4-80GB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1.147879</td>\n",
       "      <td>NVIDIA A100-SXM4-80GB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1.140018</td>\n",
       "      <td>NVIDIA A100-SXM4-80GB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1.142283</td>\n",
       "      <td>NVIDIA A100-SXM4-80GB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1.144584</td>\n",
       "      <td>NVIDIA A100-SXM4-80GB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   run  time_elapsed                 device\n",
       "0    0      1.168484  NVIDIA A100-SXM4-80GB\n",
       "1    1      1.143213  NVIDIA A100-SXM4-80GB\n",
       "2    2      1.139673  NVIDIA A100-SXM4-80GB\n",
       "3    3      1.141427  NVIDIA A100-SXM4-80GB\n",
       "4    4      1.145075  NVIDIA A100-SXM4-80GB\n",
       "5    5      1.148993  NVIDIA A100-SXM4-80GB\n",
       "6    6      1.147879  NVIDIA A100-SXM4-80GB\n",
       "7    7      1.140018  NVIDIA A100-SXM4-80GB\n",
       "8    8      1.142283  NVIDIA A100-SXM4-80GB\n",
       "9    9      1.144584  NVIDIA A100-SXM4-80GB"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "df['run'] = list(range(nruns))\n",
    "df['time_elapsed'] = list(time_dict.values())\n",
    "df['device']=torch.cuda.get_device_name(0)\n",
    "df.to_csv('pytorch_time_elapsed_'+f'{torch.cuda.get_device_name(0)}.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrypTen Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crypten\n",
    "# import torch\n",
    "\n",
    "crypten.init()\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (dropout1): Dropout(p=0.25, inplace=False)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/torch/onnx/utils.py:90: UserWarning: 'enable_onnx_checker' is deprecated and ignored. It will be removed in the next PyTorch release. To proceed despite ONNX checker failures, catch torch.onnx.ONNXCheckerError.\n",
      "  warnings.warn(\"'enable_onnx_checker' is deprecated and ignored. It will be removed in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully encrypted: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ss4yd/.local/lib/python3.8/site-packages/crypten/nn/onnx_converter.py:161: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
      "  param = torch.from_numpy(numpy_helper.to_array(node))\n"
     ]
    }
   ],
   "source": [
    "dummy_model = Net()\n",
    "plaintext_model = torch.load('./vanilla_pytorch_mnist_'+f'{torch.cuda.get_device_name(0)}.pth').to(device)\n",
    "\n",
    "print(plaintext_model)\n",
    "\n",
    "# Encrypt the model from Alice:    \n",
    "\n",
    "# 1. Create a dummy input with the same shape as the model input\n",
    "dummy_input = torch.empty((1, 1, 28, 28)).to(device)\n",
    "\n",
    "# 2. Construct a CrypTen network with the trained model and dummy_input\n",
    "private_model = crypten.nn.from_pytorch(plaintext_model, dummy_input)\n",
    "\n",
    "# 3. Encrypt the CrypTen network with src=ALICE\n",
    "private_model.encrypt(src=0)\n",
    "\n",
    "#Check that model is encrypted:\n",
    "print(\"Model successfully encrypted:\", private_model.encrypted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_enc = crypten.load_from_party('./test_data.pth')[:100].unsqueeze(1)\n",
    "# data_enc2 = data_enc[:counts]\n",
    "# data_flatten = data_enc.flatten(start_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_model.eval()\n",
    "output_enc = private_model(data_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ss4yd/.local/lib/python3.8/site-packages/crypten/encoder.py:75: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dividend = tensor // self._scale - correction\n"
     ]
    }
   ],
   "source": [
    "output = output_enc.get_plain_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 10])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-15:\n",
      "Process Process-14:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ss4yd/.local/lib/python3.8/site-packages/crypten/mpc/context.py\", line 29, in _launch\n",
      "    crypten.init()\n",
      "  File \"/home/ss4yd/.local/lib/python3.8/site-packages/crypten/mpc/context.py\", line 29, in _launch\n",
      "    crypten.init()\n",
      "  File \"/home/ss4yd/.local/lib/python3.8/site-packages/crypten/__init__.py\", line 77, in init\n",
      "    _setup_prng()\n",
      "  File \"/home/ss4yd/.local/lib/python3.8/site-packages/crypten/__init__.py\", line 77, in init\n",
      "    _setup_prng()\n",
      "  File \"/home/ss4yd/.local/lib/python3.8/site-packages/crypten/__init__.py\", line 202, in _setup_prng\n",
      "    generators[key][device] = torch.Generator(device=device)\n",
      "  File \"/home/ss4yd/.local/lib/python3.8/site-packages/crypten/__init__.py\", line 202, in _setup_prng\n",
      "    generators[key][device] = torch.Generator(device=device)\n",
      "RuntimeError: CUDA error: initialization error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "RuntimeError: CUDA error: initialization error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "ERROR:root:One of the parties failed. Check past logs\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import crypten.mpc as mpc\n",
    "import crypten.communicator as comm\n",
    "\n",
    "@mpc.run_multiprocess(world_size=2)\n",
    "def get_time_elapsed_crypten(device, test_loader):\n",
    "    plaintext_model = torch.load('./vanilla_pytorch_mnist_'+f'{torch.cuda.get_device_name(0)}.pth').to('cpu')\n",
    "    dummy_input = torch.empty((1, 1, 28, 28))\n",
    "\n",
    "    private_model = crypten.nn.from_pytorch(plaintext_model, dummy_input)\n",
    "    private_model.encrypt(src=0)\n",
    "    private_model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        t0 = time.perf_counter()\n",
    "        for data, target in tqdm(test_loader):\n",
    "            target = target\n",
    "            data_enc = crypten.cryptensor(data)\n",
    "            output_enc = private_model(data_enc)\n",
    "            output = output_enc.get_plain_text()\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        time_elapsed = time.perf_counter() - t0\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    print('Time Elapsed:{}'.format(time_elapsed))\n",
    "    return time_elapsed\n",
    "\n",
    "get_time_elapsed_crypten( device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_elapsed_crypten_gpu(device, test_loader):\n",
    "    plaintext_model = torch.load('./vanilla_pytorch_mnist_'+f'{torch.cuda.get_device_name(0)}.pth').to(device)\n",
    "    dummy_input = torch.empty((1, 1, 28, 28)).to(device)\n",
    "\n",
    "    private_model = crypten.nn.from_pytorch(plaintext_model, dummy_input)\n",
    "    private_model.encrypt(src=0)\n",
    "    private_model=private_model.to(device)\n",
    "    private_model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        t0 = time.perf_counter()\n",
    "        for data, target in tqdm(test_loader):\n",
    "            target = target.to(device)\n",
    "            data_enc = crypten.cryptensor(data).to(device)\n",
    "            output_enc = private_model(data_enc)\n",
    "            output = output_enc.get_plain_text()\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        time_elapsed = time.perf_counter() - t0\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    print('Time Elapsed:{}'.format(time_elapsed))\n",
    "    return time_elapsed\n",
    "\n",
    "get_time_elapsed_crypten_gpu(device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.10.0",
   "language": "python",
   "name": "pytorch-1.10.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
